{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from typing import Optional, Union\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "__CW = os.path.dirname(os.path.abspath(\"\")) + \"/notebooks/\"\n",
    "__SRC_FOLDER = __CW + \"../src/\"\n",
    "__DATA_FOLDER = __CW + \"../data/\"\n",
    "__DATASET_FOLDER = __DATA_FOLDER + \"datasets/\"\n",
    "__MODELS_FOLDER = __DATA_FOLDER + \"models/\"\n",
    "__OUTPUT_FOLDER = __DATA_FOLDER + \"output/\"\n",
    "\n",
    "sys.path.append(__SRC_FOLDER)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "__DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "__SEED = 42\n",
    "\n",
    "from models.transformer import VictoriaLoader\n",
    "from xai.feature_importance import GlobalExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading utilities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_model(output_file: str, model_type: str):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        output_file: The base output file name, without extension\n",
    "        model_type: The type of model: one of \"lr\", \"svm\", \"transformer\"\n",
    "\n",
    "    Returns:\n",
    "        The trained model and its training parameters, if any.\n",
    "    \"\"\"\n",
    "    if model_type == [\"lr\", \"svm\"]:\n",
    "        with open(f\"{output_file}.pickle\", \"rb\") as log:\n",
    "            model = pickle.load(log)\n",
    "    elif model_type == \"transformer\":\n",
    "        model = AutoModelForSequenceClassification()\n",
    "        model.load_state_dict(torch.load(f\"{output_file}.state_dict.pt\"))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model {model_type}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_victoria(task: Optional[str] = None, device: str = \"cuda\") -> Union[VictoriaLoader, pandas.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load the Victoria dataset.\n",
    "\n",
    "    Args:\n",
    "        task: The target task, if any\n",
    "    Returns:\n",
    "        A VictoriaLoader, if the `task` is specified, a raw DataFrame otherwise.\n",
    "    \"\"\"\n",
    "    data = pandas.read_csv(__DATASET_FOLDER + \"Gungor_2018_VictorianAuthorAttribution_data-train.csv\",\n",
    "                           encoding=\"latin-1\")\n",
    "    if task is not None:\n",
    "        return VictoriaLoader(data, task, tokenizer=AutoTokenizer.from_pretrained(\"bart-base-uncased\"), device=__DEVICE)\n",
    "    else:\n",
    "        return data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Explaining linear models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from models.preprocessing import preprocess_for_task\n",
    "from models.preprocessing import n_grams\n",
    "\n",
    "victoria_raw = load_victoria(device=__DEVICE)\n",
    "character_n_grams = 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature importance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "task = \"aa\"\n",
    "data = preprocess_for_task(victoria_raw, task,\n",
    "                           sampling_size=1.,\n",
    "                           negative_sampling_size=1.,\n",
    "                           scale_labels=True,\n",
    "                           seed=__SEED)\n",
    "data, labels, vectorizer = n_grams(data, ngrams=character_n_grams, task=task)\n",
    "\n",
    "# load model and explainer\n",
    "model = load_model(f\"{__MODELS_FOLDER}victoria_aa_svm.pickle\", \"svm\")\n",
    "explainer = GlobalExplainer(model, labels, data, task=task)\n",
    "\n",
    "# explain\n",
    "idx = 42\n",
    "x_to_explain = data[idx]\n",
    "_, explanation = explainer.explain(x_to_explain, data)\n",
    "features_by_importance = numpy.flip(numpy.argsort(explanation))\n",
    "# TODO: update w/ get_feature_names_out()\n",
    "n_grams_by_importance = [vectorizer.vocabulary_[feature] for feature in features_by_importance]\n",
    "n_gram_occurences_in_text = [[start for start in range(len(x_to_explain) - character_n_grams + 1) if data.startswith(n_gram, start)] for n_gram in n_grams_by_importance]\n",
    "nr_n_gram_occurences_in_text = [len(occurences) for occurences in n_gram_occurences_in_text]\n",
    "\n",
    "explanation_dataframe = pandas.DataFrame([(n_gram, explanation[i], i, nr_occurences) for i, (n_gram, nr_occurences) in enumerate(zip(n_grams_by_importance, nr_n_gram_occurences_in_text))],\n",
    "                                         columns=(\"n_gram\", \"importance_score\", \"importance_rank\", \"nr_occurences_in_original_document\"))\n",
    "explanation_dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = \"av\"\n",
    "data = preprocess_for_task(victoria_raw, task,\n",
    "                           sampling_size=1.,\n",
    "                           negative_sampling_size=1.,\n",
    "                           scale_labels=True,\n",
    "                           seed=__SEED)\n",
    "data, labels, vectorizer = n_grams(data, ngrams=character_n_grams, task=task)\n",
    "\n",
    "# load model and explainer\n",
    "model = load_model(f\"{__MODELS_FOLDER}victoria_av_svm\", \"svm\")\n",
    "explainer = GlobalExplainer(model, labels, data, task=task)\n",
    "\n",
    "# explain\n",
    "idx = 42\n",
    "x_to_explain = data[idx]\n",
    "_, explanation = explainer.explain(x_to_explain, data)\n",
    "features_by_importance = numpy.flip(numpy.argsort(explanation))\n",
    "n_grams_by_importance = [vectorizer.vocabulary_[feature] for feature in features_by_importance]\n",
    "n_gram_occurences_in_text = [[start for start in range(len(x_to_explain) - character_n_grams + 1) if data.startswith(n_gram, start)] for n_gram in n_grams_by_importance]\n",
    "nr_n_gram_occurences_in_text = [len(occurences) for occurences in n_gram_occurences_in_text]\n",
    "\n",
    "explanation_dataframe = pandas.DataFrame([(n_gram, explanation[i], i, nr_occurences)\n",
    "                                          for i, (n_gram, nr_occurences) in enumerate(zip(n_grams_by_importance, nr_n_gram_occurences_in_text))],\n",
    "                                         columns=(\"n_gram\", \"importance_score\", \"importance_rank\", \"nr_occurences_in_original_document\"))\n",
    "explanation_dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Counterfactual example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from xai.records import LocalCounterfactualExplainer\n",
    "\n",
    "task = \"aa\"\n",
    "data = preprocess_for_task(victoria_raw, task,\n",
    "                           sampling_size=1.,\n",
    "                           negative_sampling_size=1.,\n",
    "                           scale_labels=True,\n",
    "                           seed=__SEED)\n",
    "data, labels, vectorizer = n_grams(data, ngrams=character_n_grams, task=task)\n",
    "\n",
    "# load model and explainer\n",
    "model = load_model(f\"{__MODELS_FOLDER}victoria_aa_svm\", \"svm\")\n",
    "explainer = LocalCounterfactualExplainer(model, labels, data, task=task)\n",
    "\n",
    "# explain\n",
    "idx = 42\n",
    "x_to_explain = data[idx]\n",
    "counterfactual_x, counterfactual_x_prediction, counterfactual_distance = explainer.explain(x_to_explain, data)\n",
    "feature_differences = counterfactual_x - x_to_explain\n",
    "features_by_importance = numpy.flip(numpy.argsort(feature_differences))\n",
    "n_grams_by_importance = [vectorizer.vocabulary_[feature] for feature in features_by_importance]\n",
    "\n",
    "counterfactual_n_gram_occurences_in_text = [[start for start in range(len(x_to_explain) - character_n_grams + 1) if data.startswith(n_gram, start)] for n_gram in n_grams_by_importance]\n",
    "nr_counterfactual_n_gram_occurences_in_text = [len(occurences) for occurences in counterfactual_n_gram_occurences_in_text]\n",
    "\n",
    "explanation_dataframe = pandas.DataFrame([(n_gram, explanation[i], feature_differences[features_by_importance[i]], i, nr_occurences)\n",
    "                                          for i, (n_gram, nr_occurences) in enumerate(zip(n_grams_by_importance, nr_counterfactual_n_gram_occurences_in_text))],\n",
    "                                         columns=(\"n_gram\", \"importance_score\", \"counterfactual_distance_on_ngram\", \"importance_rank\", \"nr_occurences_in_original_document\"))\n",
    "explanation_dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Factual example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from xai.records import LocalFactualExplainer\n",
    "\n",
    "task = \"aa\"\n",
    "data = preprocess_for_task(victoria_raw, task,\n",
    "                           sampling_size=1.,\n",
    "                           negative_sampling_size=1.,\n",
    "                           scale_labels=True,\n",
    "                           seed=__SEED)\n",
    "data, labels, vectorizer = n_grams(data, ngrams=character_n_grams, task=task)\n",
    "\n",
    "# load model and explainer\n",
    "model = load_model(f\"{__MODELS_FOLDER}victoria_aa_svm\", \"svm\")\n",
    "explainer = LocalFactualExplainer(model, labels, data, task=task)\n",
    "\n",
    "# explain\n",
    "idx = 42\n",
    "x_to_explain = data[idx]\n",
    "factual_x, factual_distance = explainer.explain(x_to_explain, data)\n",
    "feature_differences = factual_x - x_to_explain\n",
    "features_by_importance = numpy.argsort(feature_differences)\n",
    "n_grams_by_importance = [vectorizer.vocabulary_[feature] for feature in features_by_importance]\n",
    "\n",
    "factual_n_gram_occurences_in_text = [[start for start in range(len(x_to_explain) - character_n_grams + 1) if data.startswith(n_gram, start)] for n_gram in n_grams_by_importance]\n",
    "nr_factual_n_gram_occurences_in_text = [len(occurences) for occurences in factual_n_gram_occurences_in_text]\n",
    "\n",
    "explanation_dataframe = pandas.DataFrame([(n_gram, explanation[i], feature_differences[features_by_importance[i]], i, nr_occurences)\n",
    "                                          for i, (n_gram, nr_occurences) in enumerate(zip(n_grams_by_importance, factual_n_gram_occurences_in_text))],\n",
    "                                         columns=(\"n_gram\", \"importance_score\", \"factual_distance_on_ngram\", \"importance_rank\", \"nr_occurences_in_original_document\"))\n",
    "explanation_dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Explaining Transformers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [2], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m task \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maa\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 2\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mload_model\u001B[49m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m__MODELS_FOLDER\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mvictoria_aa_transformer\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransformer\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbert-base-uncased\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m prober \u001B[38;5;241m=\u001B[39m TransformerProber(model, tokenizer)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import lux\n",
    "\n",
    "from xai.probing import TransformerProber\n",
    "\n",
    "task = \"aa\"\n",
    "model = load_model(f\"{__MODELS_FOLDER}victoria_aa_transformer\", \"transformer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "prober = TransformerProber(model, tokenizer)\n",
    "prober.device = __DEVICE\n",
    "victoria_dataloader = load_victoria(\"aa\", device=__DEVICE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## POS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "probe_type = \"pos\"\n",
    "k, min_len, max_len = 10, 5, 10\n",
    "n_jobs = -1\n",
    "\n",
    "# probing\n",
    "probing_results = prober.probe(victoria_dataloader,\n",
    "                               probe_type=probe_type,\n",
    "                               k=k, min_len=min_len, max_len=max_len,\n",
    "                               n_jobs=n_jobs)\n",
    "\n",
    "table_results = list()\n",
    "for author, author_probe_results in probing_results.items():\n",
    "    for i, (pos_chain, pos_chain_probing_results) in enumerate(author_probe_results):\n",
    "        probe_model, probe_configuration, probe_validation = pos_chain_probing_results\n",
    "\n",
    "        # store probing results\n",
    "        with open(f\"{__OUTPUT_FOLDER}probing_results/transformer_aa_pos_{i}.pickle\", \"wb\") as log:\n",
    "            pickle.dump(probe_model, log)\n",
    "        with open(f\"{__OUTPUT_FOLDER}probing_results/transformer_aa_pos_{i}.json\", \"w\") as log:\n",
    "            json.dump(probe_configuration, log)\n",
    "        with open(f\"{__OUTPUT_FOLDER}probing_results/transformer_aa_pos_{i}.validation.json\", \"w\") as log:\n",
    "            json.dump(probe_validation, log)\n",
    "\n",
    "        # validation dataframe\n",
    "        table_results.append([pos_chain, author,\n",
    "                              probe_validation[\"1\"][\"precision\"],\n",
    "                              probe_validation[\"1\"][\"recall\"],\n",
    "                              probe_validation[\"1\"][\"f1-score\"],\n",
    "                              probe_validation[\"1\"][\"support\"],\n",
    "                              probe_validation[\"0\"][\"precision\"],\n",
    "                              probe_validation[\"0\"][\"recall\"],\n",
    "                              probe_validation[\"0\"][\"f1-score\"],\n",
    "                              probe_validation[\"0\"][\"support\"]])\n",
    "# dump validation table\n",
    "pos_probe_columns=[\"pos_chain\", \"probed_author\",\n",
    "                   \"precision_on_probe_success\", \"recall_on_probe_success\", \"f1score_on_probe_success\", \"support_on_probe_success\",\n",
    "                   \"precision_on_probe_failure\", \"recall_on_probe_failure\", \"f1score_on_probe_failure\", \"support_on_probe_failure\"]\n",
    "results_dataframe = pandas.DataFrame(table_results, columns=pos_probe_columns)\n",
    "results_dataframe.to_csv(f\"{__OUTPUT_FOLDER}probing_results/transformer_aa_pos_probing.csv\", index=False)\n",
    "\n",
    "results_dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lemmas/Stems/NER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "probe_type = \"lemmas\"\n",
    "k, min_len, max_len = 10, 5, 10\n",
    "n_jobs = -1\n",
    "\n",
    "# probing\n",
    "probing_results = prober.probe(victoria_dataloader,\n",
    "                               probe_type=\"pos\",\n",
    "                               k=k, min_len=min_len, max_len=max_len,\n",
    "                               n_jobs=n_jobs)\n",
    "\n",
    "table_results = list()\n",
    "for author, author_probe_results in probing_results.items():\n",
    "    for i, (pos_chain, pos_chain_probing_results) in enumerate(author_probe_results):\n",
    "        probe_model, probe_configuration, probe_validation = pos_chain_probing_results\n",
    "\n",
    "        # store probing results\n",
    "        with open(f\"{__OUTPUT_FOLDER}probing_results/transformer_aa_pos_{i}.pickle\", \"wb\") as log:\n",
    "            pickle.dump(probe_model, log)\n",
    "        with open(f\"{__OUTPUT_FOLDER}probing_results/transformer_aa_pos_{i}.json\", \"w\") as log:\n",
    "            json.dump(probe_configuration, log)\n",
    "        with open(f\"{__OUTPUT_FOLDER}probing_results/transformer_aa_pos_{i}.validation.json\", \"w\") as log:\n",
    "            json.dump(probe_validation, log)\n",
    "\n",
    "        # validation dataframe\n",
    "        table_results.append([pos_chain, author,\n",
    "                              probe_validation[\"1\"][\"precision\"],\n",
    "                              probe_validation[\"1\"][\"recall\"],\n",
    "                              probe_validation[\"1\"][\"f1-score\"],\n",
    "                              probe_validation[\"1\"][\"support\"]])\n",
    "# dump validation table\n",
    "pos_probe_columns=[\"pos_chain\", \"probed_author\",\n",
    "                   \"precision_on_probe_success\", \"recall_on_probe_success\", \"f1score_on_probe_success\", \"support_on_probe_success\",\n",
    "                   \"precision_on_probe_failure\", \"recall_on_probe_failure\", \"f1score_on_probe_failure\", \"support_on_probe_failure\"]\n",
    "results_dataframe = pandas.DataFrame(table_results, columns=pos_probe_columns)\n",
    "results_dataframe.to_csv(f\"{__OUTPUT_FOLDER}probing_results/transformer_aa_pos_probing.csv\", index=False)\n",
    "\n",
    "results_dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
